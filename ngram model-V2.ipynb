{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rmukherjee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 947427\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'100% Cotton Twill.\\n5 Panel, Medium Profile.\\nUnstructured Crown & Pre-Curved Visor.\\nAdjustable Self-Material Strap With Hook And Loop Closure.\\nProtective Plastic Travel Case.\\nWorks With Most Audio Devices.\\n48\" Cord.\\nProtective Plastic Travel Case.\\nInterchangeable Earbud Covers.\\nWorks With Most Audio '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bulk\\n5-14 days\\nLarge\\nIncludes TruColor or one standard color only on one location on both gloves\\nBulk\\nInventory shown for this item may be in multiple warehouse locations. Please allow for 5-14 business days for production or call our customer service team for ship date information.\\nLarge\\n5-14 days\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"xgboost.txt\", \"r\", encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonData = pd.read_json('marketing-points.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('includes', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>marketingPoints</th>\n",
       "      <th>name</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Add a touch of personal expression to your wor...</td>\n",
       "      <td>[Inventory shown for this item may be in multi...</td>\n",
       "      <td>uni-ballÂ® 207 BLX Gel Pen</td>\n",
       "      <td>207BLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Easily slips onto the heel of your shoe Option...</td>\n",
       "      <td>[2 x CR2032 cell battery, Polybag, Offer Valid...</td>\n",
       "      <td>Cadence LED Shoe Clip</td>\n",
       "      <td>AF101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sleek, expandable zipper pouch that can fit yo...</td>\n",
       "      <td>[Includes TruColorâ„¢ on One Location Availabl...</td>\n",
       "      <td>Tempo Sports Fitness Belt</td>\n",
       "      <td>AF200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bamboo Handles Complement Metal Implements wit...</td>\n",
       "      <td>[Includes one standard color or TruColorâ„¢ on...</td>\n",
       "      <td>5 Piece Bamboo BBQ Set</td>\n",
       "      <td>BBQ25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Implements made with FDA compliant Stainless E...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5 Piece BBQ Set (Bamboo) in Roll-Up Case</td>\n",
       "      <td>BBQ28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Add a touch of personal expression to your wor...   \n",
       "1  Easily slips onto the heel of your shoe Option...   \n",
       "2  Sleek, expandable zipper pouch that can fit yo...   \n",
       "3  Bamboo Handles Complement Metal Implements wit...   \n",
       "4  Implements made with FDA compliant Stainless E...   \n",
       "\n",
       "                                     marketingPoints  \\\n",
       "0  [Inventory shown for this item may be in multi...   \n",
       "1  [2 x CR2032 cell battery, Polybag, Offer Valid...   \n",
       "2  [Includes TruColorâ„¢ on One Location Availabl...   \n",
       "3  [Includes one standard color or TruColorâ„¢ on...   \n",
       "4                                                 []   \n",
       "\n",
       "                                       name  number  \n",
       "0                uni-ballÂ® 207 BLX Gel Pen  207BLX  \n",
       "1                     Cadence LED Shoe Clip   AF101  \n",
       "2                 Tempo Sports Fitness Belt   AF200  \n",
       "3                    5 Piece Bamboo BBQ Set   BBQ25  \n",
       "4  5 Piece BBQ Set (Bamboo) in Roll-Up Case   BBQ28  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonData[\"text\"] = jsonData[\"description\"].astype(str) + \" \" +  jsonData[\"marketingPoints\"].astype(str) + \" \" + jsonData[\"name\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataV2 = '\\n'.join(jsonData['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "\n",
    "#dataV2Clean = regex.sub('\\n', dataV2)\n",
    "dataV2Clean = dataV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 1569780\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Add a touch of personal expression to your work with the rich, black-infused colors of the 207 BLX Formulated for consistently smooth, even ink flow, this series ensures that every word is brilliantly communicated uni Super Inkâ„¢ locks in ideas, protecting them from water, fading, & fraud Point siz'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"any Bottles Of Water You Drink.', 'Twist Lid Every Time Bottle Is Refilled To Update The Water Symbol On Lid.'] 22 Oz. Stainless Steel Hydro Bottle\\nPlunger Action. Rubberized Aluminum Pen. Unique Grip Design. Long Barrel With Shorter Pocket Clip. ['Long Barrel With Shorter Pocket Clip.'] Brinnix Pen\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type:\", type(dataV2Clean))\n",
    "print(\"Number of letters:\", len(dataV2Clean))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(dataV2Clean[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(dataV2Clean[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    " \n",
    "    sentences = data.split('\\n')\n",
    "   \n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "  \n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.replace('includes', '')\n",
    "        sentence = sentence.replace('plunger', '')\n",
    "        sentence = sentence.replace('laser', '')\n",
    "        sentence = sentence.replace('engraving', '')\n",
    "        \n",
    "        \n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        words=[word.lower() for word in tokenized if word.isalpha() and len(word) > 2]\n",
    "        \n",
    "        tokenized_sentences.append(words)\n",
    "    \n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "   \n",
    "    sentences = split_to_sentences(data)\n",
    "    \n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataV2Clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(dataV2Clean)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5147 data are split into 4117 train and 1030 test set\n",
      "First training sample:\n",
      "['action', 'sleek', 'write', 'low', 'viscosity', 'ink', 'smooth', 'rubberized', 'finish', 'maverick', 'sleek', 'write', 'pen']\n",
      "First test sample\n",
      "['made', 'pvc', 'nfc', 'embedded', 'and', 'easily', 'share', 'fully', 'customized', 'profiles', 'containing', 'links', 'photos', 'contact', 'information', 'and', 'more', 'use', 'linq', 'app', 'share', 'profile', 'linq', 'app', 'available', 'ios', 'android', 'full', 'color', 'linq', 'digital', 'business', 'card']\n"
     ]
    }
   ],
   "source": [
    "print(\"{} data are split into {} train and {} test set\".format(\n",
    "    len(tokenized_data), len(train_data), len(test_data)))\n",
    "\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences: \n",
    "        for token in sentence: \n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    for word, cnt in word_counts.items(): \n",
    "        if cnt >= count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    vocabulary = set(vocabulary)\n",
    "    replaced_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        for token in sentence: \n",
    "            if token in vocabulary: \n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(train_data, test_data, count_threshold):      \n",
    "    vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)    \n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data,vocabulary)    \n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data,vocabulary)\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    n_grams = {}\n",
    "    for sentence in data: \n",
    "        sentence = [start_token] * n+ sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "        for i in range(m): \n",
    "            n_gram = sentence[i:i+n]\n",
    "            if n_gram in n_grams.keys(): \n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts  else 0\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    probability = numerator / denominator\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    \n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "   \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    \n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    N = len(sentence)\n",
    "    \n",
    "   \n",
    "    product_pi = 1.0\n",
    "    \n",
    "    \n",
    "    for t in range(n, N): # complete this line\n",
    "\n",
    "        n_gram = sentence[t-n:t]\n",
    "        \n",
    "        word = sentence[t]\n",
    "       \n",
    "        probability = estimate_probability(word,n_gram, n_gram_counts, n_plus1_gram_counts, len(unique_words), k=1)\n",
    "        \n",
    "        product_pi *= 1 / probability\n",
    "\n",
    "    perplexity = product_pi**(1/float(N))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    for word, prob in probabilities.items(): # complete this line\n",
    "        if start_with != None: \n",
    "            if not word.startswith(start_with): \n",
    "                continue  \n",
    "        if prob > max_prob: \n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['action', 'sleek', 'write', 'low', 'viscosity', 'ink', 'smooth', 'rubberized', 'finish', '<unk>', 'sleek', 'write', 'pen']\n",
      "\n",
      "First preprocessed test sample:\n",
      "['made', 'pvc', 'nfc', 'embedded', 'and', 'easily', 'share', 'fully', 'customized', 'profiles', 'containing', 'links', 'photos', 'contact', 'information', 'and', 'more', 'use', 'linq', 'app', 'share', 'profile', 'linq', 'app', 'available', 'ios', 'android', 'full', 'color', 'linq', 'digital', 'business', 'card']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['action', 'sleek', 'write', 'low', 'viscosity', 'ink', 'smooth', 'rubberized', 'finish', 'pen']\n",
      "\n",
      "Size of vocabulary: 4480\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['sun'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spf', 0.003099402258135931),\n",
       " ('action', 0.00022311468094600624),\n",
       " ('action', 0.00022311468094600624),\n",
       " ('action', 0.00022311468094600624)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"sun\"]\n",
    "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['resis'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('action', 0.00022311468094600624),\n",
       " ('action', 0.00022311468094600624),\n",
       " ('action', 0.00022311468094600624),\n",
       " ('action', 0.00022311468094600624)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"resis\"]\n",
    "#tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"s\")\n",
    "tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 3\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['candy'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('made', 0.004745951982132886),\n",
       " ('<e>', 0.000860832137733142),\n",
       " ('<e>', 0.000860832137733142),\n",
       " ('<e>', 0.000860832137733142)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"\"]\n",
    "#tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"s\")\n",
    "tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"sun\", \"bum\", \"product\", \"hit\"]\n",
    "#tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"s\")\n",
    "tmp_suggest9 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('promotional', 0.021739130434782608),\n",
       " ('action', 0.00028694404591104734),\n",
       " ('action', 0.00028694404591104734),\n",
       " ('action', 0.00028694404591104734)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tmp_suggest9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reliever', 0.00846262341325811),\n",
       " ('action', 0.00028694404591104734),\n",
       " ('action', 0.00028694404591104734),\n",
       " ('action', 0.00028694404591104734)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"stress\"]\n",
    "tmp_suggest9 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "#tmp_suggest9 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "display(tmp_suggest9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
